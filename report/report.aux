\relax 
\citation{silver2016mastering}
\citation{silver2017mastering}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{peters2018deep}
\citation{radford2018improving}
\citation{wang2018glue}
\citation{rajpurkar2016squad}
\citation{rajpurkar2018know}
\citation{devlin2018bert}
\citation{liu2019multi}
\citation{liu2019improving}
\citation{yang2019xlnet}
\citation{hochreiter1997long}
\citation{chung2014empirical}
\citation{vaswani2017attention}
\citation{radford2018improving}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}\protected@file@percent }
\newlabel{sec:background}{{2}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Transformer}{1}\protected@file@percent }
\newlabel{subsec:transformer}{{2.1}{1}}
\citation{liu2018generating}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{devlin2018bert}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Transformer Architecture}}{2}\protected@file@percent }
\newlabel{fig:transformer_architecture}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (left) Scaled Dot-Product Attention; (right) Multi-Head Attention}}{2}\protected@file@percent }
\newlabel{fig:transformer_attention}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}GPT}{2}\protected@file@percent }
\newlabel{subsec:gpt}{{2.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces BERT Input Representation}}{2}\protected@file@percent }
\newlabel{fig:bert_input_representation}{{3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}BERT}{2}\protected@file@percent }
\newlabel{subsec:bert}{{2.3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{2}\protected@file@percent }
\newlabel{sec:implementation}{{3}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Results}{3}\protected@file@percent }
\newlabel{sec:experimental_results}{{4}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Prediction Results}}{3}\protected@file@percent }
\newlabel{tab:prediction_results}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Best Prediction Result(ensemble)}}{3}\protected@file@percent }
\newlabel{fig:best_ensemble}{{4}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{3}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{3}}
\bibstyle{IEEEtran}
\bibdata{reference}
\bibcite{silver2016mastering}{1}
\bibcite{silver2017mastering}{2}
\bibcite{mikolov2013efficient}{3}
\bibcite{pennington2014glove}{4}
\bibcite{peters2018deep}{5}
\bibcite{radford2018improving}{6}
\bibcite{wang2018glue}{7}
\bibcite{rajpurkar2016squad}{8}
\bibcite{rajpurkar2018know}{9}
\bibcite{devlin2018bert}{10}
\bibcite{liu2019multi}{11}
\bibcite{liu2019improving}{12}
\bibcite{yang2019xlnet}{13}
\bibcite{hochreiter1997long}{14}
\bibcite{chung2014empirical}{15}
\bibcite{vaswani2017attention}{16}
\bibcite{liu2018generating}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Best Prediction Result(single)}}{4}\protected@file@percent }
\newlabel{fig:best_single}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Evaluation Accuracy}}{4}\protected@file@percent }
\newlabel{fig:eval_acc}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Evaluation Loss}}{4}\protected@file@percent }
\newlabel{fig:eval_loss}{{7}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Train Loss}}{4}\protected@file@percent }
\newlabel{fig:train_loss}{{8}{4}}
